{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import dask.bag as db\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, hamming_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding,  Dropout,  SpatialDropout1D, LSTM\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sklearn.metrics as mt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "data_file = '/Users/leventguner/Downloads/LeventMac/paper_data/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "def get_metadata():\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define text cleaner\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data with dask\n",
    "docs = db.read_text(data_file).map(json.loads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': '0704.0001',\n",
       "  'submitter': 'Pavel Nadolsky',\n",
       "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       "  'comments': '37 pages, 15 figures; published version',\n",
       "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       "  'doi': '10.1103/PhysRevD.76.013009',\n",
       "  'report-no': 'ANL-HEP-PR-07-12',\n",
       "  'categories': 'hep-ph',\n",
       "  'license': None,\n",
       "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       "  'update_date': '2008-11-26',\n",
       "  'authors_parsed': [['Balázs', 'C.', ''],\n",
       "   ['Berger', 'E. L.', ''],\n",
       "   ['Nadolsky', 'P. M.', ''],\n",
       "   ['Yuan', 'C. -P.', '']]},)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see an instance example\n",
    "docs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get latest versions and convert to df\n",
    "\n",
    "get_latest_version = lambda x: x['versions'][-1]['created']\n",
    "\n",
    "# get only necessary fields\n",
    "trim = lambda x: {'id': x['id'],\n",
    "                  'title': x['title'],\n",
    "                  'category':x['categories'].split(' '),\n",
    "                  'abstract':x['abstract']}\n",
    "# filter for papers published on or after 2019-01-01\n",
    "columns = ['id','category','abstract']\n",
    "docs_df = (docs\n",
    "             .filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)\n",
    "             .map(trim)\n",
    "             .compute())\n",
    "\n",
    "# convert to pandas\n",
    "docs_df = pd.DataFrame(docs_df)\n",
    "\n",
    "# add general category. we are going to use as our target variable\n",
    "docs_df['general_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['sub_category'] = docs_df.category.apply(lambda x:[a.split('.')[1] if ('.' in a) else a.split('.')[0]+'_nsc' for a in x])\n",
    "docs_df['new_category'] = docs_df.category.apply(lambda x:[[a.split('.')[0],a.split('.')[1]] if ('.' in a) else [a.split('.')[0],a.split('.')[0]+'_nsc'] for a in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                         1907.10813\n",
       "title               Model-independent reconstruction of $f(T)$ gra...\n",
       "category                         [astro-ph.CO, gr-qc, hep-ph, hep-th]\n",
       "abstract              We apply Gaussian processes and Hubble funct...\n",
       "general_category                    [astro-ph, gr-qc, hep-ph, hep-th]\n",
       "sub_category                  [CO, gr-qc_nsc, hep-ph_nsc, hep-th_nsc]\n",
       "new_category        [[astro-ph, CO], [gr-qc, gr-qc_nsc], [hep-ph, ...\n",
       "Name: 128310, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.iloc[128310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>abstract</th>\n",
       "      <th>general_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>new_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.1445</td>\n",
       "      <td>Deformed Wigner crystal in a one-dimensional q...</td>\n",
       "      <td>[cond-mat.str-el, cond-mat.mes-hall]</td>\n",
       "      <td>The spatial Fourier spectrum of the electron...</td>\n",
       "      <td>[cond-mat, cond-mat]</td>\n",
       "      <td>[str-el, mes-hall]</td>\n",
       "      <td>[[cond-mat, str-el], [cond-mat, mes-hall]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0705.0033</td>\n",
       "      <td>Ergodic Theory: Recurrence</td>\n",
       "      <td>[math.DS]</td>\n",
       "      <td>We survey the impact of the Poincar\\'e recur...</td>\n",
       "      <td>[math]</td>\n",
       "      <td>[DS]</td>\n",
       "      <td>[[math, DS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0705.0344</td>\n",
       "      <td>Unifying derived deformation theories</td>\n",
       "      <td>[math.AG]</td>\n",
       "      <td>We develop a framework for derived deformati...</td>\n",
       "      <td>[math]</td>\n",
       "      <td>[AG]</td>\n",
       "      <td>[[math, AG]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0705.0825</td>\n",
       "      <td>Einstein's Theory of Gravity in the Presence o...</td>\n",
       "      <td>[gr-qc, astro-ph, hep-th]</td>\n",
       "      <td>The mysterious `dark energy' needed to expla...</td>\n",
       "      <td>[gr-qc, astro-ph, hep-th]</td>\n",
       "      <td>[gr-qc_nsc, astro-ph_nsc, hep-th_nsc]</td>\n",
       "      <td>[[gr-qc, gr-qc_nsc], [astro-ph, astro-ph_nsc],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0705.2562</td>\n",
       "      <td>Anthropic prediction in a large toy landscape</td>\n",
       "      <td>[hep-th]</td>\n",
       "      <td>The successful anthropic prediction of the c...</td>\n",
       "      <td>[hep-th]</td>\n",
       "      <td>[hep-th_nsc]</td>\n",
       "      <td>[[hep-th, hep-th_nsc]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324653</th>\n",
       "      <td>quant-ph/0612050</td>\n",
       "      <td>The exact cost of redistributing multipartite ...</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>How correlated are two quantum systems from ...</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>[quant-ph_nsc]</td>\n",
       "      <td>[[quant-ph, quant-ph_nsc]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324654</th>\n",
       "      <td>quant-ph/0701163</td>\n",
       "      <td>Does Observation Create Reality?</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>It has been suggested that the locality of i...</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>[quant-ph_nsc]</td>\n",
       "      <td>[[quant-ph, quant-ph_nsc]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324655</th>\n",
       "      <td>quant-ph/0702160</td>\n",
       "      <td>Discrete-query quantum algorithm for NAND trees</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>Recently, Farhi, Goldstone, and Gutmann gave...</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>[quant-ph_nsc]</td>\n",
       "      <td>[[quant-ph, quant-ph_nsc]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324656</th>\n",
       "      <td>quant-ph/9606017</td>\n",
       "      <td>Quantum Mechanics in Terms of Realism</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>.We expound an alternative to the Copenhagen...</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>[quant-ph_nsc]</td>\n",
       "      <td>[[quant-ph, quant-ph_nsc]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324657</th>\n",
       "      <td>quant-ph/9806088</td>\n",
       "      <td>Quantum Games and Quantum Strategies</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>We investigate the quantization of non-zero ...</td>\n",
       "      <td>[quant-ph]</td>\n",
       "      <td>[quant-ph_nsc]</td>\n",
       "      <td>[[quant-ph, quant-ph_nsc]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324658 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  \\\n",
       "0              0704.1445  Deformed Wigner crystal in a one-dimensional q...   \n",
       "1              0705.0033                         Ergodic Theory: Recurrence   \n",
       "2              0705.0344              Unifying derived deformation theories   \n",
       "3              0705.0825  Einstein's Theory of Gravity in the Presence o...   \n",
       "4              0705.2562      Anthropic prediction in a large toy landscape   \n",
       "...                  ...                                                ...   \n",
       "324653  quant-ph/0612050  The exact cost of redistributing multipartite ...   \n",
       "324654  quant-ph/0701163                   Does Observation Create Reality?   \n",
       "324655  quant-ph/0702160    Discrete-query quantum algorithm for NAND trees   \n",
       "324656  quant-ph/9606017              Quantum Mechanics in Terms of Realism   \n",
       "324657  quant-ph/9806088               Quantum Games and Quantum Strategies   \n",
       "\n",
       "                                    category  \\\n",
       "0       [cond-mat.str-el, cond-mat.mes-hall]   \n",
       "1                                  [math.DS]   \n",
       "2                                  [math.AG]   \n",
       "3                  [gr-qc, astro-ph, hep-th]   \n",
       "4                                   [hep-th]   \n",
       "...                                      ...   \n",
       "324653                            [quant-ph]   \n",
       "324654                            [quant-ph]   \n",
       "324655                            [quant-ph]   \n",
       "324656                            [quant-ph]   \n",
       "324657                            [quant-ph]   \n",
       "\n",
       "                                                 abstract  \\\n",
       "0         The spatial Fourier spectrum of the electron...   \n",
       "1         We survey the impact of the Poincar\\'e recur...   \n",
       "2         We develop a framework for derived deformati...   \n",
       "3         The mysterious `dark energy' needed to expla...   \n",
       "4         The successful anthropic prediction of the c...   \n",
       "...                                                   ...   \n",
       "324653    How correlated are two quantum systems from ...   \n",
       "324654    It has been suggested that the locality of i...   \n",
       "324655    Recently, Farhi, Goldstone, and Gutmann gave...   \n",
       "324656    .We expound an alternative to the Copenhagen...   \n",
       "324657    We investigate the quantization of non-zero ...   \n",
       "\n",
       "                 general_category                           sub_category  \\\n",
       "0            [cond-mat, cond-mat]                     [str-el, mes-hall]   \n",
       "1                          [math]                                   [DS]   \n",
       "2                          [math]                                   [AG]   \n",
       "3       [gr-qc, astro-ph, hep-th]  [gr-qc_nsc, astro-ph_nsc, hep-th_nsc]   \n",
       "4                        [hep-th]                           [hep-th_nsc]   \n",
       "...                           ...                                    ...   \n",
       "324653                 [quant-ph]                         [quant-ph_nsc]   \n",
       "324654                 [quant-ph]                         [quant-ph_nsc]   \n",
       "324655                 [quant-ph]                         [quant-ph_nsc]   \n",
       "324656                 [quant-ph]                         [quant-ph_nsc]   \n",
       "324657                 [quant-ph]                         [quant-ph_nsc]   \n",
       "\n",
       "                                             new_category  \n",
       "0              [[cond-mat, str-el], [cond-mat, mes-hall]]  \n",
       "1                                            [[math, DS]]  \n",
       "2                                            [[math, AG]]  \n",
       "3       [[gr-qc, gr-qc_nsc], [astro-ph, astro-ph_nsc],...  \n",
       "4                                  [[hep-th, hep-th_nsc]]  \n",
       "...                                                   ...  \n",
       "324653                         [[quant-ph, quant-ph_nsc]]  \n",
       "324654                         [[quant-ph, quant-ph_nsc]]  \n",
       "324655                         [[quant-ph, quant-ph_nsc]]  \n",
       "324656                         [[quant-ph, quant-ph_nsc]]  \n",
       "324657                         [[quant-ph, quant-ph_nsc]]  \n",
       "\n",
       "[324658 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df2 = docs_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare categories for prediction\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(docs_df2.general_category)\n",
    "\n",
    "mlb_sub = MultiLabelBinarizer()\n",
    "labels_sub = mlb_sub.fit_transform(docs_df2.sub_category)\n",
    "labels_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted\n",
      "cleaned\n",
      "tokenized\n"
     ]
    }
   ],
   "source": [
    "y = labels\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(docs_df2.abstract, y, test_size=0.33, random_state=42)\n",
    "print('splitted')\n",
    "\n",
    "#clean\n",
    "ct = CleanText()\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_test = ct.transform(X_test)\n",
    "\n",
    "print('cleaned')\n",
    "\n",
    "\n",
    "#tokenization\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(X_train) #sadece train ile yap\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) #train ve test için ayrı ayrı yap\n",
    "X_train = pad_sequences(X_train,maxlen=217) #train ve test için ayrı ayrı yap\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test) #train ve test için ayrı ayrı yap\n",
    "X_test = pad_sequences(X_test,maxlen=217) #train ve test için ayrı ayrı yap\n",
    "\n",
    "print('tokenized')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 217, 32)           320000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 217, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                1720      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                220       \n",
      "=================================================================\n",
      "Total params: 321,940\n",
      "Trainable params: 321,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "210/210 [==============================] - 98s 468ms/step - loss: 3.9390 - accuracy: 0.2527 - val_loss: 3.6491 - val_accuracy: 0.2221\n",
      "Epoch 2/2\n",
      "210/210 [==============================] - 88s 421ms/step - loss: 3.5326 - accuracy: 0.3060 - val_loss: 3.5699 - val_accuracy: 0.3464\n"
     ]
    }
   ],
   "source": [
    "#create the LSTM model\n",
    "embed_dim = 32\n",
    "lstm_out = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(y_train.shape[1],activation='sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# fit model\n",
    "batch_size = 32\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 2, batch_size=batch_size, verbose = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
